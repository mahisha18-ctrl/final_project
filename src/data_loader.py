"""
Data Loader for Travel Documents
RUBRIC: Data Loading & Preparation (10 marks total)
- TravelDataLoader implemented correctly (3 marks)
- PDFs loaded directly from data/ folder (3 marks)
- Document categorization based on filename (2 marks)
- Text chunking configured properly (2 marks)

TASK: Load PDFs from data/ folder, categorize them, and chunk them
"""
import os
import pandas as pd
from pathlib import Path
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Data directory should point to "data"
DATA_DIR = Path("data")


class TravelDataLoader:
    """Loads travel documents (PDFs, CSVs) from data directory"""

    def __init__(self):
        # Initialize text splitter with chunk_size=1000, chunk_overlap=200
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )

    def load_pdfs_from_data_directory(self) -> List[Document]:
        """
        Load all PDFs directly from data/ directory

        This method:
        1. Checks if DATA_DIR exists
        2. Gets all PDF files using glob
        3. Loops through files and loads with PyPDFLoader
        4. Adds metadata (source, file_type, category)
        5. Returns list of documents
        """
        documents = []

        # Check if DATA_DIR exists
        if not DATA_DIR.exists():
            print(f"Warning: Directory {DATA_DIR} does not exist")
            return documents

        # Get all PDF files using glob pattern "*.pdf"
        pdf_files = list(DATA_DIR.glob("*.pdf"))
        print(f"Found {len(pdf_files)} PDF files in data/")

        for pdf_file in pdf_files:
            try:
                # Use PyPDFLoader to load the PDF
                loader = PyPDFLoader(str(pdf_file))
                docs = loader.load()

                # Add metadata to each document
                for doc in docs:
                    doc.metadata.update({
                        'source': pdf_file.name,
                        'file_type': "pdf",
                        'category': self._categorize_document(pdf_file.name)
                    })

                documents.extend(docs)
                print(f"  âœ“ Loaded: {pdf_file.name} ({len(docs)} pages)")

            except Exception as e:
                print(f"  âœ— Error loading {pdf_file.name}: {e}")

        return documents

    def _categorize_document(self, filename: str) -> str:
        """Categorize document based on filename"""
        filename_lower = filename.lower()

        if "air-india" in filename_lower or "ai-schedule" in filename_lower:
            return "air_india_policies"
        elif "u.s. department" in filename_lower or "transportation" in filename_lower:
            return "us_dot_regulations"
        elif "booking" in filename_lower or "policy" in filename_lower:
            return "booking_policies"
        elif "refund" in filename_lower:
            return "refund_policies"
        elif "privacy" in filename_lower:
            return "privacy_policies"
        else:
            return "general"

    def load_csvs_from_data_directory(self) -> List[Document]:
        """
        Load all CSVs directly from data/ directory

        Similar to PDF loading but for CSV files
        """
        documents = []

        if not DATA_DIR.exists():
            print(f"Warning: Directory {DATA_DIR} does not exist")
            return documents

        # Get all CSV files
        csv_files = list(DATA_DIR.glob("*.csv"))
        print(f"Found {len(csv_files)} CSV files in data/")

        for csv_file in csv_files:
            try:
                # Use pandas to read CSV
                df = pd.read_csv(csv_file)

                # Convert each row to a Document
                for idx, row in df.iterrows():
                    # Create content by joining column:value pairs
                    content = " | ".join(
                        f"{col}: {val}" for col, val in row.items()
                    )

                    # Create Document with metadata
                    documents.append(
                        Document(
                            page_content=content,
                            metadata={
                                'source': csv_file.name,
                                'file_type': "csv",
                                'row_index': idx,
                                'category': self._categorize_document(csv_file.name)
                            }
                        )
                    )

                print(f"  âœ“ Loaded: {csv_file.name} ({len(df)} rows)")

            except Exception as e:
                print(f"  âœ— Error loading {csv_file.name}: {e}")

        return documents

    def load_all_travel_documents(self) -> List[Document]:
        """
        Load all PDFs and CSVs from data directory

        Calls both PDF and CSV loaders and combines results
        """
        all_documents = []

        print("\nğŸ“‚ Loading travel knowledge base...")
        print("=" * 60)

        # Load PDFs
        pdf_docs = self.load_pdfs_from_data_directory()
        all_documents.extend(pdf_docs)

        # Load CSVs
        csv_docs = self.load_csvs_from_data_directory()
        all_documents.extend(csv_docs)

        print("=" * 60)
        print(f"âœ… Total documents loaded: {len(all_documents)}")

        return all_documents

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        Split documents into chunks

        Uses self.text_splitter.split_documents()
        """
        print(f"\nâœ‚ï¸  Splitting {len(documents)} documents into chunks...")

        # Use self.text_splitter to split documents
        chunks = self.text_splitter.split_documents(documents)

        print(f"âœ… Created {len(chunks)} chunks")
        print(
            f"   Average chunk size: "
            f"{sum(len(c.page_content) for c in chunks) // max(len(chunks), 1)} chars"
        )

        return chunks


if __name__ == "__main__":
    loader = TravelDataLoader()
    docs = loader.load_all_travel_documents()
    chunks = loader.split_documents(docs)

    print("\nğŸ“Š Summary:")
    print(f"   Total chunks: {len(chunks)}")